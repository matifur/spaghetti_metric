# ðŸ“Š Scalable Metric for Evaluating LLMs in Code Interpretation

This repository contains the implementation of a **scalable evaluation metric** designed to assess large language models (LLMs) in terms of their ability to **interpret complex Python programs**. The project provides an objective tool for measuring LLMs' accuracy in understanding and analyzing source code, focusing on both **syntactic correctness** and **functional validity**.

## ðŸš€ Features
- **Evaluation of LLMs** in Python code interpretation tasks.
- **Comparison of different models**, including OpenAI's GPT-3.5 Turbo and MetaAI's LLaMA.
- **Custom metric** for assessing correctness, functionality, and semantic accuracy.
- **Scalable and adaptable** for testing models on various programming challenges.

## ðŸ“– Future Work
- Expanding the metric to **other programming languages**.
- Enhancing evaluation criteria to cover **more complex programming paradigms**.
- Improving scoring mechanisms for **better semantic understanding assessment**.


